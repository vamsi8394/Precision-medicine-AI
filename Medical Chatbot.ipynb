{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMfhso0dVOngtJm4FZfFTJb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vamsi8394/Precision-medicine-AI/blob/main/Medical%20Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# --- Configuration ---\n",
        "# Ensure the API key is set\n",
        "if not GOOGLE_API_KEY:\n",
        "    st.error(\"GOOGLE_API_KEY not found in .env file. Please set it up.\")\n",
        "    st.stop()\n",
        "\n",
        "# Initialize Google Generative AI components\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3, google_api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def get_pdf_text(pdf_docs):\n",
        "    \"\"\"Extracts text from a list of PDF documents.\"\"\"\n",
        "    text = \"\"\n",
        "    for pdf in pdf_docs:\n",
        "        pdf_reader = PdfReader(pdf)\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text() or \"\" # Ensure text is not None\n",
        "    return text\n",
        "\n",
        "def get_text_chunks(text):\n",
        "    \"\"\"Splits text into manageable chunks.\"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len\n",
        "    )\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "def get_vector_store(text_chunks):\n",
        "    \"\"\"Creates a FAISS vector store from text chunks using Google embeddings.\"\"\"\n",
        "    # Note: For a real application, you might want to save/load this to disk\n",
        "    # using FAISS.save_local and FAISS.load_local for persistence.\n",
        "    # For this demo, it's in-memory.\n",
        "    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
        "    return vector_store\n",
        "\n",
        "def get_conversation_chain(vector_store):\n",
        "    \"\"\"Creates a conversational retrieval chain with memory.\"\"\"\n",
        "    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vector_store.as_retriever(),\n",
        "        memory=memory\n",
        "    )\n",
        "    return conversation_chain\n",
        "\n",
        "def handle_userinput(user_question):\n",
        "    \"\"\"Processes user questions and displays responses.\"\"\"\n",
        "    if st.session_state.conversation:\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            response = st.session_state.conversation({'question': user_question})\n",
        "        st.session_state.chat_history = response['chat_history']\n",
        "\n",
        "        # Display chat history\n",
        "        for i, message in enumerate(st.session_state.chat_history):\n",
        "            if i % 2 == 0: # User message\n",
        "                with st.chat_message(\"user\"):\n",
        "                    st.write(message.content)\n",
        "            else: # AI message\n",
        "                with st.chat_message(\"assistant\"):\n",
        "                    st.write(message.content)\n",
        "    else:\n",
        "        st.warning(\"Please upload PDF documents first to start the conversation.\")\n",
        "\n",
        "# --- Streamlit UI ---\n",
        "\n",
        "def main():\n",
        "    st.set_page_config(page_title=\"Medical AI Assistant\", page_icon=\"ðŸ©º\")\n",
        "\n",
        "    st.title(\"ðŸ©º Medical AI Assistant\")\n",
        "    st.markdown(\"Upload medical PDFs and ask questions about their content.\")\n",
        "\n",
        "    # Initialize session state variables if they don't exist\n",
        "    if \"conversation\" not in st.session_state:\n",
        "        st.session_state.conversation = None\n",
        "    if \"chat_history\" not in st.session_state:\n",
        "        st.session_state.chat_history = []\n",
        "    if \"processed_pdfs\" not in st.session_state:\n",
        "        st.session_state.processed_pdfs = False\n",
        "\n",
        "    # Sidebar for PDF upload\n",
        "    with st.sidebar:\n",
        "        st.header(\"Your Documents\")\n",
        "        pdf_docs = st.file_uploader(\n",
        "            \"Upload your medical PDFs here and click 'Process'\",\n",
        "            accept_multiple_files=True,\n",
        "            type=[\"pdf\"]\n",
        "        )\n",
        "        if st.button(\"Process Documents\"):\n",
        "            if pdf_docs:\n",
        "                with st.spinner(\"Processing... This may take a moment.\"):\n",
        "                    # Get PDF text\n",
        "                    raw_text = get_pdf_text(pdf_docs)\n",
        "\n",
        "                    # Get text chunks\n",
        "                    text_chunks = get_text_chunks(raw_text)\n",
        "\n",
        "                    # Create vector store\n",
        "                    vector_store = get_vector_store(text_chunks)\n",
        "\n",
        "                    # Create conversation chain\n",
        "                    st.session_state.conversation = get_conversation_chain(vector_store)\n",
        "                    st.session_state.processed_pdfs = True\n",
        "                    st.success(\"Documents processed! You can now ask questions.\")\n",
        "                    st.session_state.chat_history = [] # Clear history on new processing\n",
        "            else:\n",
        "                st.warning(\"Please upload at least one PDF document.\")\n",
        "\n",
        "    # Main chat interface\n",
        "    if st.session_state.processed_pdfs:\n",
        "        st.subheader(\"Ask a question about your documents:\")\n",
        "\n",
        "        # Display existing chat history\n",
        "        for i, message in enumerate(st.session_state.chat_history):\n",
        "            if i % 2 == 0: # User message\n",
        "                with st.chat_message(\"user\"):\n",
        "                    st.write(message.content)\n",
        "            else: # AI message\n",
        "                with st.chat_message(\"assistant\"):\n",
        "                    st.write(message.content)\n",
        "\n",
        "        user_question = st.chat_input(\"Type your question here...\")\n",
        "        if user_question:\n",
        "            handle_userinput(user_question)\n",
        "    else:\n",
        "        st.info(\"Upload PDF documents in the sidebar to begin.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JxnqjV-qAfA",
        "outputId": "d4b51d85-919b-4a2f-e177-dae70a7ddad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Done! Ready to download.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"my-medical-chatbot.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "-vtD11tYqHSZ",
        "outputId": "f49bcfa4-db06-4ba8-c981-9d3f8d869059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_70b43013-01f3-49c3-9200-6c3768696963\", \"my-medical-chatbot.zip\", 1205)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}